{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1LjMNLgcR9_CNJwmN615MtpixWSozZTpj","authorship_tag":"ABX9TyN5CQDdDWus15/fwwAmWZJV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e35ab98442b14148a9225a513b931e87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c23beaec9a6f46518b13c5ce67a998b6","IPY_MODEL_f8254c74fcdf4e4b9ec4e90d077580a4","IPY_MODEL_53860af8bb3b444297302e9add1e4539"],"layout":"IPY_MODEL_88db814fe533423da03fe82f2e401e63"}},"c23beaec9a6f46518b13c5ce67a998b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea4150f746fa41f495751063816d0a99","placeholder":"​","style":"IPY_MODEL_d3fdc5ab03b744919cf32c9a41345310","value":"GroundingDINO_SwinB.cfg.py: 100%"}},"f8254c74fcdf4e4b9ec4e90d077580a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3344a38767147f08b2fe82e73ec3624","max":1007,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e63a7ca79814444c9219444658c43f89","value":1007}},"53860af8bb3b444297302e9add1e4539":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e489dc58cefa4fb8bdf2cbaaa8f971ea","placeholder":"​","style":"IPY_MODEL_ca8ec67b714e42d7803abcbd9432a31c","value":" 1.01k/1.01k [00:00&lt;00:00, 66.9kB/s]"}},"88db814fe533423da03fe82f2e401e63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea4150f746fa41f495751063816d0a99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3fdc5ab03b744919cf32c9a41345310":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3344a38767147f08b2fe82e73ec3624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e63a7ca79814444c9219444658c43f89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e489dc58cefa4fb8bdf2cbaaa8f971ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca8ec67b714e42d7803abcbd9432a31c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88ebac158dc74d4289739f454fe72077":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ef0133d2a554a5595f10431b957568e","IPY_MODEL_bccc8cc5d61f444bb88dbb026c4d5427","IPY_MODEL_8e2fdab1f31c4d9cbe237115395365c1"],"layout":"IPY_MODEL_6a348ba83fdc41e983108a4fc21f016a"}},"2ef0133d2a554a5595f10431b957568e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2cbfe0ebbefe461c8f59facd5a8e726b","placeholder":"​","style":"IPY_MODEL_950b1d8a0c084ff2809d886af28a94ce","value":"tokenizer_config.json: 100%"}},"bccc8cc5d61f444bb88dbb026c4d5427":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a658af056e84651bcbd0a5a12891183","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6e124584cde41f1ba3c391ff9899321","value":48}},"8e2fdab1f31c4d9cbe237115395365c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e48ced6111ee4b3598c8faff0dd522cf","placeholder":"​","style":"IPY_MODEL_1ac4af6a2b0d42afbde0aeb65226abfe","value":" 48.0/48.0 [00:00&lt;00:00, 1.36kB/s]"}},"6a348ba83fdc41e983108a4fc21f016a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cbfe0ebbefe461c8f59facd5a8e726b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"950b1d8a0c084ff2809d886af28a94ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a658af056e84651bcbd0a5a12891183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6e124584cde41f1ba3c391ff9899321":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e48ced6111ee4b3598c8faff0dd522cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ac4af6a2b0d42afbde0aeb65226abfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e722b3558a6d4b9cb23297e386790e80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0dd72bb2ebe04ac8aa88e64593c94747","IPY_MODEL_3db420d40ca84116b219e16e45fa6bff","IPY_MODEL_0fc44ec4be934e6096b3582a52fe27cc"],"layout":"IPY_MODEL_094189d1be7f488a9f893188b70f7d53"}},"0dd72bb2ebe04ac8aa88e64593c94747":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f065bf998290482690cafbf30b149778","placeholder":"​","style":"IPY_MODEL_e515a6c4224e4ddabb93e36d2f708e72","value":"config.json: 100%"}},"3db420d40ca84116b219e16e45fa6bff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_951b68563a1b4f25b866bda8f8b52430","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2677832d9f74b6ab7d588a7e5171435","value":570}},"0fc44ec4be934e6096b3582a52fe27cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c38d565de3c4e99894eb5141d3d02d5","placeholder":"​","style":"IPY_MODEL_1bd582040c6347699a7b3fe82e4a4d61","value":" 570/570 [00:00&lt;00:00, 14.6kB/s]"}},"094189d1be7f488a9f893188b70f7d53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f065bf998290482690cafbf30b149778":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e515a6c4224e4ddabb93e36d2f708e72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"951b68563a1b4f25b866bda8f8b52430":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2677832d9f74b6ab7d588a7e5171435":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c38d565de3c4e99894eb5141d3d02d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bd582040c6347699a7b3fe82e4a4d61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60a129eb2e0a44febb2f9119437d70fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b17c4eeb9c314f4eb47aa0eda7c3a0b6","IPY_MODEL_76f1afbe86b347d5bf224d43d3bbea02","IPY_MODEL_b277eb2d9d564edea19dab120ef1112b"],"layout":"IPY_MODEL_2617d90e8777472fb76d4dbf6144e290"}},"b17c4eeb9c314f4eb47aa0eda7c3a0b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_733d9de5be644391866896917e661984","placeholder":"​","style":"IPY_MODEL_6df353fd9e3b4460bbd91a2f7f5d811d","value":"vocab.txt: 100%"}},"76f1afbe86b347d5bf224d43d3bbea02":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5aac7caa20547b5979af01ca3f0e005","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86489b4877284ca0a8e246831180dff5","value":231508}},"b277eb2d9d564edea19dab120ef1112b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e10e3015294446e292fd5ed1a716c19d","placeholder":"​","style":"IPY_MODEL_6e732a8b8fb24c9796f56bafb730115f","value":" 232k/232k [00:00&lt;00:00, 2.25MB/s]"}},"2617d90e8777472fb76d4dbf6144e290":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"733d9de5be644391866896917e661984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6df353fd9e3b4460bbd91a2f7f5d811d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5aac7caa20547b5979af01ca3f0e005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86489b4877284ca0a8e246831180dff5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e10e3015294446e292fd5ed1a716c19d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e732a8b8fb24c9796f56bafb730115f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6b4902d10594bb4bfb17194e598a230":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f45061b24f2d45e19aacda9e79d8ffcb","IPY_MODEL_7c61a137377149139f21a6e013a2b280","IPY_MODEL_48cb5dcf0f184af88287ebce41cbef76"],"layout":"IPY_MODEL_3558864eec534b46af95231269b64a2b"}},"f45061b24f2d45e19aacda9e79d8ffcb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b19695bf88542008cd9642635cd2e79","placeholder":"​","style":"IPY_MODEL_6d60f6fb42db4e4ba08298c56b7ec54d","value":"tokenizer.json: 100%"}},"7c61a137377149139f21a6e013a2b280":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8985b0b920d8445da2813135bd836013","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb3ca13fa52441c6956c1a1709c4bdb8","value":466062}},"48cb5dcf0f184af88287ebce41cbef76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fafbb17eeb8d4f7bbf6a56f1f966ff74","placeholder":"​","style":"IPY_MODEL_3684210cc9a2492080de07a543f590e2","value":" 466k/466k [00:00&lt;00:00, 4.74MB/s]"}},"3558864eec534b46af95231269b64a2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b19695bf88542008cd9642635cd2e79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d60f6fb42db4e4ba08298c56b7ec54d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8985b0b920d8445da2813135bd836013":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb3ca13fa52441c6956c1a1709c4bdb8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fafbb17eeb8d4f7bbf6a56f1f966ff74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3684210cc9a2492080de07a543f590e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7a9f2c9a25c4e2ebfeb615ace44a662":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52b16ca119994acb8573e579019020f6","IPY_MODEL_31e7fd83b64d4d3e86e6360c3939657a","IPY_MODEL_5497ab040b9f4321b78f74ec0498ab6a"],"layout":"IPY_MODEL_17d8546218654aad887241d0e3801113"}},"52b16ca119994acb8573e579019020f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0eb106feb8434ceea88eb1234ba549f5","placeholder":"​","style":"IPY_MODEL_da8ba4085876415fb27b674674f39270","value":"model.safetensors: 100%"}},"31e7fd83b64d4d3e86e6360c3939657a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9207246a692447889d571feddb3b1b3","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_173d8b5a10fc490fb72c63dda9f61ab5","value":440449768}},"5497ab040b9f4321b78f74ec0498ab6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecb55c205f6e4708a20d352b77d46bdb","placeholder":"​","style":"IPY_MODEL_0bc9681899af41faaed859363feddb29","value":" 440M/440M [00:03&lt;00:00, 156MB/s]"}},"17d8546218654aad887241d0e3801113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eb106feb8434ceea88eb1234ba549f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da8ba4085876415fb27b674674f39270":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9207246a692447889d571feddb3b1b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"173d8b5a10fc490fb72c63dda9f61ab5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecb55c205f6e4708a20d352b77d46bdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bc9681899af41faaed859363feddb29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"beef57661f454dd8854d0f1b2b7e7b53":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66574e1bf5f646efbe0aa36f41d36e89","IPY_MODEL_2a9f5c968e5048069daece2a9a19eb21","IPY_MODEL_0911db8501aa445aa2557b2691ab2a42"],"layout":"IPY_MODEL_bf0fbf08eebc4876b4a3f90a1402b062"}},"66574e1bf5f646efbe0aa36f41d36e89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_266e26f67ea24499aeb3321997afca04","placeholder":"​","style":"IPY_MODEL_811002c72363455a9c5ad63474462353","value":"groundingdino_swinb_cogcoor.pth: 100%"}},"2a9f5c968e5048069daece2a9a19eb21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0e474119c8745c9bc260bae9a86428b","max":938057991,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24a65857d3204a62913b2a30a8efe758","value":938057991}},"0911db8501aa445aa2557b2691ab2a42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_156832b7d5b14b578b717eb8af70374e","placeholder":"​","style":"IPY_MODEL_ae3e7cf37e874d4a94401d608ba6b42a","value":" 938M/938M [00:07&lt;00:00, 258MB/s]"}},"bf0fbf08eebc4876b4a3f90a1402b062":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"266e26f67ea24499aeb3321997afca04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"811002c72363455a9c5ad63474462353":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0e474119c8745c9bc260bae9a86428b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24a65857d3204a62913b2a30a8efe758":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"156832b7d5b14b578b717eb8af70374e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae3e7cf37e874d4a94401d608ba6b42a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# !mkdir /root/.ssh\n","# !cp /content/drive/MyDrive/deploy_keys/id_ed25519* /root/.ssh/\n","# !ssh-keyscan -t ed25519 github.com >> ~/.ssh/known_hosts\n","# !ssh -T git@github.com"],"metadata":{"id":"C2dQiFLTvNHS","executionInfo":{"status":"ok","timestamp":1729686099961,"user_tz":-180,"elapsed":812,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/ddudakov/DatasetEditing.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2dvo2bs4LPS","executionInfo":{"status":"ok","timestamp":1729686103978,"user_tz":-180,"elapsed":3568,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"069c7b54-99fb-47a0-a961-9dc5311060b5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'DatasetEditing'...\n","remote: Enumerating objects: 500, done.\u001b[K\n","remote: Counting objects: 100% (500/500), done.\u001b[K\n","remote: Compressing objects: 100% (435/435), done.\u001b[K\n","remote: Total 500 (delta 53), reused 489 (delta 42), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (500/500), 17.07 MiB | 10.76 MiB/s, done.\n","Resolving deltas: 100% (53/53), done.\n"]}]},{"cell_type":"markdown","source":["***Установка пакетов***"],"metadata":{"id":"9xY5ilBq0Y03"}},{"cell_type":"code","source":["cd DatasetEditing"],"metadata":{"id":"9pdtm15B4Tcu","executionInfo":{"status":"ok","timestamp":1729686122921,"user_tz":-180,"elapsed":1524,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"62985a19-2567-43a7-e6a9-cdca3888d8aa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'DatasetEditing'\n","/content/DatasetEditing\n"]}]},{"cell_type":"code","source":["#!pip install git+ssh://git@github.com/ddudakov/DatasetEditing.git"],"metadata":{"id":"krHSz3671TlP","executionInfo":{"status":"ok","timestamp":1729686123314,"user_tz":-180,"elapsed":2,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os, sys\n","\n","sys.path.append(os.path.join(os.getcwd(), \"SAM/segment_anything\"))\n","sys.path.append(os.path.join(os.getcwd(), \"GroundingDINO\"))\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""],"metadata":{"id":"F2bdIZdXNlIz","executionInfo":{"status":"ok","timestamp":1729686123314,"user_tz":-180,"elapsed":2,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(sys.path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3n9xknWdC1i","executionInfo":{"status":"ok","timestamp":1729686124026,"user_tz":-180,"elapsed":3,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"340ddbf6-b647-4148-97eb-50ba089a9924"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/usr/local/lib/python3.10/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/DatasetEditing/SAM/segment_anything', '/content/DatasetEditing/GroundingDINO']\n"]}]},{"cell_type":"code","source":["!python -m pip install -e GroundingDINO"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BXl2_hQUPQVg","executionInfo":{"status":"ok","timestamp":1729686224913,"user_tz":-180,"elapsed":100132,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"56b1f142-811d-47bd-90eb-30ce49e99592"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/DatasetEditing/GroundingDINO\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.4.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.19.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (4.44.2)\n","Collecting addict (from groundingdino==0.1.0)\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Collecting yapf (from groundingdino==0.1.0)\n","  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (1.0.11)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (1.26.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (4.10.0.84)\n","Collecting supervision>=0.22.0 (from groundingdino==0.1.0)\n","  Downloading supervision-0.24.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.0.8)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (10.4.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.35.1)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.8.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.24.0)\n","Collecting kornia (from groundingdino==0.1.0)\n","  Downloading kornia-0.7.3-py2.py3-none-any.whl.metadata (7.7 kB)\n","Collecting dominate (from groundingdino==0.1.0)\n","  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n","Collecting lpips (from groundingdino==0.1.0)\n","  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n","Collecting Ninja (from groundingdino==0.1.0)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n","Collecting ftfy (from groundingdino==0.1.0)\n","  Downloading ftfy-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2024.9.11)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (3.7.1)\n","Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.5.4)\n","Collecting gradio>=5 (from groundingdino==0.1.0)\n","  Downloading gradio-5.3.0-py3-none-any.whl.metadata (15 kB)\n","Collecting omegaconf (from groundingdino==0.1.0)\n","  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (3.7.1)\n","Collecting fastapi<1.0,>=0.115.2 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n","Collecting ffmpy (from gradio>=5->groundingdino==0.1.0)\n","  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting gradio-client==1.4.2 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting httpx>=0.24.1 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting huggingface-hub>=0.25.1 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (3.1.4)\n","Collecting markupsafe~=2.0 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Collecting orjson~=3.0 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (24.1)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (2.2.2)\n","Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (2.9.2)\n","Collecting pydub (from gradio>=5->groundingdino==0.1.0)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart>=0.0.9 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (6.0.2)\n","Collecting ruff>=0.2.2 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading ruff-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting starlette<1.0,>=0.40.0 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n","Collecting tomlkit==0.12.0 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (0.12.5)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=5->groundingdino==0.1.0) (4.12.2)\n","Collecting uvicorn>=0.14.0 (from gradio>=5->groundingdino==0.1.0)\n","  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio>=5->groundingdino==0.1.0) (2024.6.1)\n","Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio>=5->groundingdino==0.1.0)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (0.7.1)\n","Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision>=0.22.0->groundingdino==0.1.0) (1.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->groundingdino==0.1.0) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->groundingdino==0.1.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->groundingdino==0.1.0) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->groundingdino==0.1.0) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->groundingdino==0.1.0) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->groundingdino==0.1.0) (2.8.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->groundingdino==0.1.0) (0.2.13)\n","Collecting kornia-rs>=0.1.0 (from kornia->groundingdino==0.1.0)\n","  Downloading kornia_rs-0.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.16.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.4.1)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips->groundingdino==0.1.0) (4.66.5)\n","Collecting antlr4-python3-runtime==4.9.* (from omegaconf->groundingdino==0.1.0)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->groundingdino==0.1.0) (2024.9.20)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->groundingdino==0.1.0) (0.4)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->groundingdino==0.1.0) (0.4.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (0.19.1)\n","Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (8.5.0)\n","Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (4.3.6)\n","Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (2.0.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=5->groundingdino==0.1.0) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=5->groundingdino==0.1.0) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=5->groundingdino==0.1.0) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=5->groundingdino==0.1.0) (2024.8.30)\n","Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=5->groundingdino==0.1.0)\n","  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio>=5->groundingdino==0.1.0)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->groundingdino==0.1.0) (3.20.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio>=5->groundingdino==0.1.0) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio>=5->groundingdino==0.1.0) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio>=5->groundingdino==0.1.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio>=5->groundingdino==0.1.0) (2.23.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->groundingdino==0.1.0) (1.16.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=5->groundingdino==0.1.0) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=5->groundingdino==0.1.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=5->groundingdino==0.1.0) (13.9.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (2.2.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->groundingdino==0.1.0) (1.3.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5->groundingdino==0.1.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5->groundingdino==0.1.0) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=5->groundingdino==0.1.0) (0.1.2)\n","Downloading gradio-5.3.0-py3-none-any.whl (56.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading supervision-0.24.0-py3-none-any.whl (158 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Downloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n","Downloading ftfy-6.3.0-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kornia-0.7.3-py2.py3-none-any.whl (833 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m833.3/833.3 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yapf-0.40.2-py3-none-any.whl (254 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.26.1-py3-none-any.whl (447 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.4/447.4 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kornia_rs-0.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n","Downloading ruff-0.7.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=da92d0554359fcc374adad7db6540c49174967bedcc5c369b9bc841f139b61ed\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: pydub, Ninja, antlr4-python3-runtime, addict, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, omegaconf, markupsafe, kornia-rs, h11, ftfy, ffmpy, dominate, aiofiles, yapf, uvicorn, starlette, huggingface-hub, httpcore, supervision, httpx, fastapi, kornia, gradio-client, lpips, gradio, groundingdino\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.24.7\n","    Uninstalling huggingface-hub-0.24.7:\n","      Successfully uninstalled huggingface-hub-0.24.7\n","  Running setup.py develop for groundingdino\n","Successfully installed Ninja-1.11.1.1 addict-2.4.0 aiofiles-23.2.1 antlr4-python3-runtime-4.9.3 dominate-2.9.1 fastapi-0.115.3 ffmpy-0.4.0 ftfy-6.3.0 gradio-5.3.0 gradio-client-1.4.2 groundingdino-0.1.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.1 kornia-0.7.3 kornia-rs-0.1.5 lpips-0.1.4 markupsafe-2.1.5 omegaconf-2.3.0 orjson-3.10.10 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.0 semantic-version-2.10.0 starlette-0.41.0 supervision-0.24.0 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0 yapf-0.40.2\n"]}]},{"cell_type":"code","source":["import argparse\n","import os\n","import copy\n","\n","import numpy as np\n","import torch\n","from PIL import Image, ImageDraw, ImageFont\n","from torchvision.ops import box_convert\n","\n","# Grounding DINO\n","from GroundingDINO.groundingdino.datasets import transforms as T\n","from GroundingDINO.groundingdino.models import build_model\n","from GroundingDINO.groundingdino.util import box_ops\n","from GroundingDINO.groundingdino.util.slconfig import SLConfig\n","from GroundingDINO.groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n","from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n","\n","import supervision as sv\n","# segment anything\n","from SAM.segment_anything import build_sam, SamPredictor\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","import PIL\n","import requests\n","import torch\n","from io import BytesIO\n","\n","\n","\n","from huggingface_hub import hf_hub_download"],"metadata":{"id":"UDU_Zowa0fsu","executionInfo":{"status":"ok","timestamp":1729686240351,"user_tz":-180,"elapsed":15441,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df016b24-e580-4eb3-c9fe-c413542e9c93"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n","  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"]}]},{"cell_type":"markdown","source":["**Загружаем Grounding DINO**"],"metadata":{"id":"m8BklKXC37OD"}},{"cell_type":"code","source":["def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n","    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n","\n","    args = SLConfig.fromfile(cache_config_file)\n","    model = build_model(args)\n","    args.device = device\n","\n","    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n","    checkpoint = torch.load(cache_file, map_location='cpu')\n","    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n","    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n","    _ = model.eval()\n","    return model"],"metadata":{"id":"hCWwV3-536p-","executionInfo":{"status":"ok","timestamp":1729686240352,"user_tz":-180,"elapsed":6,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n","ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n","ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n","\n","groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filename, ckpt_config_filename)"],"metadata":{"id":"xoYruzsB0-Nz","colab":{"base_uri":"https://localhost:8080/","height":461,"referenced_widgets":["e35ab98442b14148a9225a513b931e87","c23beaec9a6f46518b13c5ce67a998b6","f8254c74fcdf4e4b9ec4e90d077580a4","53860af8bb3b444297302e9add1e4539","88db814fe533423da03fe82f2e401e63","ea4150f746fa41f495751063816d0a99","d3fdc5ab03b744919cf32c9a41345310","b3344a38767147f08b2fe82e73ec3624","e63a7ca79814444c9219444658c43f89","e489dc58cefa4fb8bdf2cbaaa8f971ea","ca8ec67b714e42d7803abcbd9432a31c","88ebac158dc74d4289739f454fe72077","2ef0133d2a554a5595f10431b957568e","bccc8cc5d61f444bb88dbb026c4d5427","8e2fdab1f31c4d9cbe237115395365c1","6a348ba83fdc41e983108a4fc21f016a","2cbfe0ebbefe461c8f59facd5a8e726b","950b1d8a0c084ff2809d886af28a94ce","7a658af056e84651bcbd0a5a12891183","d6e124584cde41f1ba3c391ff9899321","e48ced6111ee4b3598c8faff0dd522cf","1ac4af6a2b0d42afbde0aeb65226abfe","e722b3558a6d4b9cb23297e386790e80","0dd72bb2ebe04ac8aa88e64593c94747","3db420d40ca84116b219e16e45fa6bff","0fc44ec4be934e6096b3582a52fe27cc","094189d1be7f488a9f893188b70f7d53","f065bf998290482690cafbf30b149778","e515a6c4224e4ddabb93e36d2f708e72","951b68563a1b4f25b866bda8f8b52430","b2677832d9f74b6ab7d588a7e5171435","9c38d565de3c4e99894eb5141d3d02d5","1bd582040c6347699a7b3fe82e4a4d61","60a129eb2e0a44febb2f9119437d70fe","b17c4eeb9c314f4eb47aa0eda7c3a0b6","76f1afbe86b347d5bf224d43d3bbea02","b277eb2d9d564edea19dab120ef1112b","2617d90e8777472fb76d4dbf6144e290","733d9de5be644391866896917e661984","6df353fd9e3b4460bbd91a2f7f5d811d","f5aac7caa20547b5979af01ca3f0e005","86489b4877284ca0a8e246831180dff5","e10e3015294446e292fd5ed1a716c19d","6e732a8b8fb24c9796f56bafb730115f","a6b4902d10594bb4bfb17194e598a230","f45061b24f2d45e19aacda9e79d8ffcb","7c61a137377149139f21a6e013a2b280","48cb5dcf0f184af88287ebce41cbef76","3558864eec534b46af95231269b64a2b","1b19695bf88542008cd9642635cd2e79","6d60f6fb42db4e4ba08298c56b7ec54d","8985b0b920d8445da2813135bd836013","cb3ca13fa52441c6956c1a1709c4bdb8","fafbb17eeb8d4f7bbf6a56f1f966ff74","3684210cc9a2492080de07a543f590e2","f7a9f2c9a25c4e2ebfeb615ace44a662","52b16ca119994acb8573e579019020f6","31e7fd83b64d4d3e86e6360c3939657a","5497ab040b9f4321b78f74ec0498ab6a","17d8546218654aad887241d0e3801113","0eb106feb8434ceea88eb1234ba549f5","da8ba4085876415fb27b674674f39270","b9207246a692447889d571feddb3b1b3","173d8b5a10fc490fb72c63dda9f61ab5","ecb55c205f6e4708a20d352b77d46bdb","0bc9681899af41faaed859363feddb29","beef57661f454dd8854d0f1b2b7e7b53","66574e1bf5f646efbe0aa36f41d36e89","2a9f5c968e5048069daece2a9a19eb21","0911db8501aa445aa2557b2691ab2a42","bf0fbf08eebc4876b4a3f90a1402b062","266e26f67ea24499aeb3321997afca04","811002c72363455a9c5ad63474462353","d0e474119c8745c9bc260bae9a86428b","24a65857d3204a62913b2a30a8efe758","156832b7d5b14b578b717eb8af70374e","ae3e7cf37e874d4a94401d608ba6b42a"]},"executionInfo":{"status":"ok","timestamp":1729686259952,"user_tz":-180,"elapsed":19605,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"58f7abaa-7d14-4bc2-9cd1-15a8c2970b7e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n"]},{"output_type":"display_data","data":{"text/plain":["GroundingDINO_SwinB.cfg.py:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e35ab98442b14148a9225a513b931e87"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n"]},{"output_type":"stream","name":"stdout","text":["final text_encoder_type: bert-base-uncased\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ebac158dc74d4289739f454fe72077"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e722b3558a6d4b9cb23297e386790e80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60a129eb2e0a44febb2f9119437d70fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b4902d10594bb4bfb17194e598a230"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7a9f2c9a25c4e2ebfeb615ace44a662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["groundingdino_swinb_cogcoor.pth:   0%|          | 0.00/938M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beef57661f454dd8854d0f1b2b7e7b53"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded from /root/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n"," => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"]}]},{"cell_type":"markdown","source":["**Загружаем SAM**"],"metadata":{"id":"0NKEZIz4dayD"}},{"cell_type":"code","source":["! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"],"metadata":{"id":"k2VGB08Q4Mor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729686279963,"user_tz":-180,"elapsed":20015,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"cad50f7e-9d3e-4a2e-e816-1656a0d8d779"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-10-23 12:24:19--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.172.185.63, 18.172.185.104, 18.172.185.43, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.172.185.63|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2564550879 (2.4G) [binary/octet-stream]\n","Saving to: ‘sam_vit_h_4b8939.pth’\n","\n","sam_vit_h_4b8939.pt 100%[===================>]   2.39G   217MB/s    in 20s     \n","\n","2024-10-23 12:24:39 (125 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n","\n"]}]},{"cell_type":"code","source":["DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","sam_checkpoint = 'sam_vit_h_4b8939.pth'\n","sam = build_sam(checkpoint=sam_checkpoint)\n","sam.to(device=DEVICE)\n","sam_predictor = SamPredictor(sam)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ITjHD6DdfxS","executionInfo":{"status":"ok","timestamp":1729686297912,"user_tz":-180,"elapsed":17952,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"d0ef6ea7-f983-4a8f-f396-ad700115ace3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"]}]},{"cell_type":"markdown","source":["**Загружаем checkpoints для Background Editing и checkpoints для inpaint-anything**"],"metadata":{"id":"EX5jTmdsdlir"}},{"cell_type":"code","source":["import zipfile\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","# https://drive.google.com/drive/folders/1hVh-eLY_J5TEryOtkofMpjgZ6ANu9r09?usp=sharing\n","# background editing\n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/ckpt_editing/checkpoints.zip\", 'r')\n","zip_ref.extractall(\"/content/DatasetEditing/editing_diffusion/\")\n","zip_ref.close()\n","\n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/ckpt_editing/checkpoints1.zip\", 'r')\n","zip_ref.extractall(\"/content/DatasetEditing/editing_diffusion/\")\n","zip_ref.close()\n","\n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/ckpt_editing/ViT-B-16.zip\", 'r')\n","zip_ref.extractall(\"/root/.cache/clip/\")\n","zip_ref.close()\n","\n","# inpaint anything\n","!cp -r '/content/drive/MyDrive/Colab Notebooks/pretrained_models' '/content/DatasetEditing/Inpaint-Anything/'"],"metadata":{"id":"tU0z0D2Gdj5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729686412292,"user_tz":-180,"elapsed":114387,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"b1cb2333-96ef-4b1c-be14-96879fe475f8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["**Загрузим датасет для экспериментов**"],"metadata":{"id":"cGxLJSiwPkee"}},{"cell_type":"code","source":["!rm -rf /content/sample_data && mkdir /content/sample_data && mkdir /content/sample_data/test\n","\n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/test.zip\", 'r')\n","zip_ref.extractall(\"/content/sample_data/test\")\n","zip_ref.close()\n","\n","#TEXT_PROMPT = \"bird\"\n","BOX_TRESHOLD = 0.3\n","TEXT_TRESHOLD = 0.25"],"metadata":{"id":"ahaYQXrZPjW_","executionInfo":{"status":"ok","timestamp":1729686414645,"user_tz":-180,"elapsed":2358,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["!rm -rf /content/sample_data/test_new\n","!mkdir /content/sample_data/test_new\n","# !cp -r /content/sample_data/test/EMU /content/sample_data/test_new\n","# !cp -r /content/sample_data/test/GRANDALA /content/sample_data/test_new\n","# !cp -r /content/sample_data/test/BOBOLINK /content/sample_data/test_new\n","# !cp -r /content/sample_data/test/SORA /content/sample_data/test_new\n","!cp -r /content/sample_data/test/ANIANIAU /content/sample_data/test_new\n","!cp -r /content/sample_data/test/CASSOWARY /content/sample_data/test_new"],"metadata":{"id":"IkqCwsEJKqeJ","executionInfo":{"status":"ok","timestamp":1729686414646,"user_tz":-180,"elapsed":4,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\""],"metadata":{"id":"LKRMerYgT4xl","executionInfo":{"status":"ok","timestamp":1729686414646,"user_tz":-180,"elapsed":3,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# TESTING\n","import random\n","import os\n","from pathlib import Path\n","import subprocess\n","TEXT_PROMPT = \"bird\"\n","\n","dir_img_path = \"/content/sample_data/test_new\"\n","names = sorted([name for name in os.listdir(dir_img_path)])\n","scales = [0.05, 0.08, 0.11]\n","for name, scale in zip(names, scales):\n","  class_path = f\"{dir_img_path}/{name}\"\n","  TEXT_PROMPT = name\n","  prelim = f\"{class_path}/preliminaries\"\n","  os.makedirs(prelim, exist_ok=True)\n","  for img in os.listdir(class_path):\n","    filepath = os.path.join(class_path, img)\n","    if os.path.isfile(filepath):\n","        img_path = f\"{class_path}/{img}\"\n","        image_source, image = load_image(img_path)\n","        boxes, logits, phrases = predict(\n","            model=groundingdino_model,\n","            image=image,\n","            caption=TEXT_PROMPT,\n","            box_threshold=BOX_TRESHOLD,\n","            text_threshold=TEXT_TRESHOLD,\n","            device=DEVICE\n","        )\n","        # set image\n","        sam_predictor.set_image(image_source)\n","        # box: normalized box xywh -> unnormalized xyxy\n","        H, W, _ = image_source.shape\n","        boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n","        transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_xyxy, image_source.shape[:2]).to(DEVICE)\n","        masks, _, _ = sam_predictor.predict_torch(\n","                    point_coords = None,\n","                    point_labels = None,\n","                    boxes = transformed_boxes,\n","                    multimask_output = False,\n","                    )\n","        mask_path = f\"{prelim}/{Path(img).stem}_mask.png\"\n","        image_mask = masks[0][0].cpu().numpy()\n","        image_mask_pil = Image.fromarray(image_mask) # our mask\n","        image_mask_pil.save(mask_path)\n","        !python /content/DatasetEditing/Inpaint-Anything/remove_anything.py --input_img {img_path} --input_mask {mask_path} --dilate_kernel_size 15 \\\n","        --output_dir {prelim} --lama_config \"/content/DatasetEditing/Inpaint-Anything/lama_configs/default.yaml\" --lama_ckpt \"/content/DatasetEditing/Inpaint-Anything/pretrained_models/big-lama\"\n","        !python /content/DatasetEditing/resize_obj.py --img_path {img_path} --mask_path {mask_path} --output_path {prelim} --scale {scale}\n","        # load img_rescaled and mask_rescaled\n","        background_path = f\"{prelim}/{Path(img).stem}_mask_background.png\"\n","        img_rescaled_path = f\"{prelim}/{Path(img).stem}_rescaled.png\"\n","        mask_rescaled_path = f\"{prelim}/{Path(img).stem}_mask_rescaled.png\"\n","        !python /content/DatasetEditing/editing_diffusion/main.py -p \"\" --batch_size 1 -i {background_path} -i2 {img_rescaled_path}  \\\n","        --mask {mask_rescaled_path} --output_path {class_path} --output_file {Path(img)} --skip_timesteps 0 --iterations_num 5 --invert_mask --classifier_scale 0. --y 0 --final_save_root {class_path} --background_complex 15 --hard --random_position --coarse_to_fine #--background_preservation_loss # --vid\n"],"metadata":{"id":"A4lxgIgMSFiW","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1729686638282,"user_tz":-180,"elapsed":223639,"user":{"displayName":"Денис Дудаков","userId":"02015614028601239611"}},"outputId":"4c116289-e7e0-4999-ae13-3b18158cffca"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/DatasetEditing/Inpaint-Anything/remove_anything.py\", line 5, in <module>\n","    from lama_inpaint import inpaint_img_with_lama\n","  File \"/content/DatasetEditing/Inpaint-Anything/lama_inpaint.py\", line 19, in <module>\n","    from saicinpainting.evaluation.utils import move_to_device\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/__init__.py\", line 6, in <module>\n","    from saicinpainting.evaluation.losses.base_loss import SSIMScore, LPIPSScore, FIDScore\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/losses/base_loss.py\", line 13, in <module>\n","    from models.ade20k import SegmentationModule, NUM_CLASS, segm_options\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/__init__.py\", line 1, in <module>\n","    from .base import *\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/base.py\", line 22, in <module>\n","    classes=pd.read_csv(classes_path),)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n","    parser = TextFileReader(filepath_or_buffer, **kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n","    self._engine = self._make_engine(f, self.engine)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n","    self.handles = get_handle(\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 873, in get_handle\n","    handle = open(\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/DatasetEditing/Inpaint-Anything/models/ade20k/object150_info.csv'\n","0.050123565051020405\n","/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","Namespace(prompt='', init_image='/content/sample_data/test_new/ANIANIAU/preliminaries/1_mask_background.png', init_image_2='/content/sample_data/test_new/ANIANIAU/preliminaries/1_rescaled.png', mask='/content/sample_data/test_new/ANIANIAU/preliminaries/1_mask_rescaled.png', skip_timesteps=0, local_clip_guided_diffusion=False, ddim=False, timestep_respacing='100', model_output_size=256, aug_num=8, clip_guidance_lambda=1000, range_lambda=50, lpips_sim_lambda=1000, l2_sim_lambda=10000, background_preservation_loss=False, invert_mask=True, enforce_background=True, seed=404, gpu_id=0, output_path='/content/sample_data/test_new/ANIANIAU', output_file='1.jpg', iterations_num=5, batch_size=1, save_video=False, export_assets=False, image_guide=False, coarse_to_fine=True, classifier_scale=0.0, y=0, class_cond=False, background_complex=15.0, final_save_root='/content/sample_data/test_new/ANIANIAU', hard=True, random_position=True, rotate_obj=False, angle=0)\n","Using device: cuda:0\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\"/content/DatasetEditing/editing_diffusion/checkpoints/256x256_classifier.pt\", map_location=\"cpu\")\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100% 528M/528M [00:03<00:00, 174MB/s]\n","Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n","/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/editing_diffusion/main.py\", line 8, in <module>\n","    image_editor.edit_image_by_prompt()\n","  File \"/content/DatasetEditing/editing_diffusion/optimization/image_editor.py\", line 188, in edit_image_by_prompt\n","    self.init_image_pil = Image.open(self.args.init_image).convert(\"RGB\")\n","  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3431, in open\n","    fp = builtins.open(filename, \"rb\")\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/sample_data/test_new/ANIANIAU/preliminaries/1_mask_background.png'\n"]},{"output_type":"stream","name":"stderr","text":["FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/content/DatasetEditing/Inpaint-Anything/remove_anything.py\", line 5, in <module>\n","    from lama_inpaint import inpaint_img_with_lama\n","  File \"/content/DatasetEditing/Inpaint-Anything/lama_inpaint.py\", line 19, in <module>\n","    from saicinpainting.evaluation.utils import move_to_device\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/__init__.py\", line 6, in <module>\n","    from saicinpainting.evaluation.losses.base_loss import SSIMScore, LPIPSScore, FIDScore\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/losses/base_loss.py\", line 13, in <module>\n","    from models.ade20k import SegmentationModule, NUM_CLASS, segm_options\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/__init__.py\", line 1, in <module>\n","    from .base import *\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/base.py\", line 22, in <module>\n","    classes=pd.read_csv(classes_path),)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n","    parser = TextFileReader(filepath_or_buffer, **kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n","    self._engine = self._make_engine(f, self.engine)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n","    self.handles = get_handle(\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 873, in get_handle\n","    handle = open(\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/DatasetEditing/Inpaint-Anything/models/ade20k/object150_info.csv'\n","0.05020328443877551\n","/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","Namespace(prompt='', init_image='/content/sample_data/test_new/ANIANIAU/preliminaries/3_mask_background.png', init_image_2='/content/sample_data/test_new/ANIANIAU/preliminaries/3_rescaled.png', mask='/content/sample_data/test_new/ANIANIAU/preliminaries/3_mask_rescaled.png', skip_timesteps=0, local_clip_guided_diffusion=False, ddim=False, timestep_respacing='100', model_output_size=256, aug_num=8, clip_guidance_lambda=1000, range_lambda=50, lpips_sim_lambda=1000, l2_sim_lambda=10000, background_preservation_loss=False, invert_mask=True, enforce_background=True, seed=404, gpu_id=0, output_path='/content/sample_data/test_new/ANIANIAU', output_file='3.jpg', iterations_num=5, batch_size=1, save_video=False, export_assets=False, image_guide=False, coarse_to_fine=True, classifier_scale=0.0, y=0, class_cond=False, background_complex=15.0, final_save_root='/content/sample_data/test_new/ANIANIAU', hard=True, random_position=True, rotate_obj=False, angle=0)\n","Using device: cuda:0\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\"/content/DatasetEditing/editing_diffusion/checkpoints/256x256_classifier.pt\", map_location=\"cpu\")\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n","/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/editing_diffusion/main.py\", line 8, in <module>\n","    image_editor.edit_image_by_prompt()\n","  File \"/content/DatasetEditing/editing_diffusion/optimization/image_editor.py\", line 188, in edit_image_by_prompt\n","    self.init_image_pil = Image.open(self.args.init_image).convert(\"RGB\")\n","  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3431, in open\n","    fp = builtins.open(filename, \"rb\")\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/sample_data/test_new/ANIANIAU/preliminaries/3_mask_background.png'\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/Inpaint-Anything/remove_anything.py\", line 5, in <module>\n","    from lama_inpaint import inpaint_img_with_lama\n","  File \"/content/DatasetEditing/Inpaint-Anything/lama_inpaint.py\", line 19, in <module>\n","    from saicinpainting.evaluation.utils import move_to_device\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/__init__.py\", line 6, in <module>\n","    from saicinpainting.evaluation.losses.base_loss import SSIMScore, LPIPSScore, FIDScore\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/losses/base_loss.py\", line 13, in <module>\n","    from models.ade20k import SegmentationModule, NUM_CLASS, segm_options\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/__init__.py\", line 1, in <module>\n","    from .base import *\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/base.py\", line 22, in <module>\n","    classes=pd.read_csv(classes_path),)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n","    parser = TextFileReader(filepath_or_buffer, **kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n","    self._engine = self._make_engine(f, self.engine)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n","    self.handles = get_handle(\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 873, in get_handle\n","    handle = open(\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/DatasetEditing/Inpaint-Anything/models/ade20k/object150_info.csv'\n","0.05006377551020408\n","/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","Namespace(prompt='', init_image='/content/sample_data/test_new/ANIANIAU/preliminaries/2_mask_background.png', init_image_2='/content/sample_data/test_new/ANIANIAU/preliminaries/2_rescaled.png', mask='/content/sample_data/test_new/ANIANIAU/preliminaries/2_mask_rescaled.png', skip_timesteps=0, local_clip_guided_diffusion=False, ddim=False, timestep_respacing='100', model_output_size=256, aug_num=8, clip_guidance_lambda=1000, range_lambda=50, lpips_sim_lambda=1000, l2_sim_lambda=10000, background_preservation_loss=False, invert_mask=True, enforce_background=True, seed=404, gpu_id=0, output_path='/content/sample_data/test_new/ANIANIAU', output_file='2.jpg', iterations_num=5, batch_size=1, save_video=False, export_assets=False, image_guide=False, coarse_to_fine=True, classifier_scale=0.0, y=0, class_cond=False, background_complex=15.0, final_save_root='/content/sample_data/test_new/ANIANIAU', hard=True, random_position=True, rotate_obj=False, angle=0)\n","Using device: cuda:0\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\"/content/DatasetEditing/editing_diffusion/checkpoints/256x256_classifier.pt\", map_location=\"cpu\")\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n","/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/editing_diffusion/main.py\", line 8, in <module>\n","    image_editor.edit_image_by_prompt()\n","  File \"/content/DatasetEditing/editing_diffusion/optimization/image_editor.py\", line 188, in edit_image_by_prompt\n","    self.init_image_pil = Image.open(self.args.init_image).convert(\"RGB\")\n","  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3431, in open\n","    fp = builtins.open(filename, \"rb\")\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/sample_data/test_new/ANIANIAU/preliminaries/2_mask_background.png'\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/Inpaint-Anything/remove_anything.py\", line 5, in <module>\n","    from lama_inpaint import inpaint_img_with_lama\n","  File \"/content/DatasetEditing/Inpaint-Anything/lama_inpaint.py\", line 19, in <module>\n","    from saicinpainting.evaluation.utils import move_to_device\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/__init__.py\", line 6, in <module>\n","    from saicinpainting.evaluation.losses.base_loss import SSIMScore, LPIPSScore, FIDScore\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/losses/base_loss.py\", line 13, in <module>\n","    from models.ade20k import SegmentationModule, NUM_CLASS, segm_options\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/__init__.py\", line 1, in <module>\n","    from .base import *\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/base.py\", line 22, in <module>\n","    classes=pd.read_csv(classes_path),)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n","    parser = TextFileReader(filepath_or_buffer, **kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n","    self._engine = self._make_engine(f, self.engine)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n","    self.handles = get_handle(\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 873, in get_handle\n","    handle = open(\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/DatasetEditing/Inpaint-Anything/models/ade20k/object150_info.csv'\n","0.04956552933673469\n","/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","Namespace(prompt='', init_image='/content/sample_data/test_new/ANIANIAU/preliminaries/4_mask_background.png', init_image_2='/content/sample_data/test_new/ANIANIAU/preliminaries/4_rescaled.png', mask='/content/sample_data/test_new/ANIANIAU/preliminaries/4_mask_rescaled.png', skip_timesteps=0, local_clip_guided_diffusion=False, ddim=False, timestep_respacing='100', model_output_size=256, aug_num=8, clip_guidance_lambda=1000, range_lambda=50, lpips_sim_lambda=1000, l2_sim_lambda=10000, background_preservation_loss=False, invert_mask=True, enforce_background=True, seed=404, gpu_id=0, output_path='/content/sample_data/test_new/ANIANIAU', output_file='4.jpg', iterations_num=5, batch_size=1, save_video=False, export_assets=False, image_guide=False, coarse_to_fine=True, classifier_scale=0.0, y=0, class_cond=False, background_complex=15.0, final_save_root='/content/sample_data/test_new/ANIANIAU', hard=True, random_position=True, rotate_obj=False, angle=0)\n","Using device: cuda:0\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\"/content/DatasetEditing/editing_diffusion/checkpoints/256x256_classifier.pt\", map_location=\"cpu\")\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n","/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/editing_diffusion/main.py\", line 8, in <module>\n","    image_editor.edit_image_by_prompt()\n","  File \"/content/DatasetEditing/editing_diffusion/optimization/image_editor.py\", line 188, in edit_image_by_prompt\n","    self.init_image_pil = Image.open(self.args.init_image).convert(\"RGB\")\n","  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3431, in open\n","    fp = builtins.open(filename, \"rb\")\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/sample_data/test_new/ANIANIAU/preliminaries/4_mask_background.png'\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/Inpaint-Anything/remove_anything.py\", line 5, in <module>\n","    from lama_inpaint import inpaint_img_with_lama\n","  File \"/content/DatasetEditing/Inpaint-Anything/lama_inpaint.py\", line 19, in <module>\n","    from saicinpainting.evaluation.utils import move_to_device\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/__init__.py\", line 6, in <module>\n","    from saicinpainting.evaluation.losses.base_loss import SSIMScore, LPIPSScore, FIDScore\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/losses/base_loss.py\", line 13, in <module>\n","    from models.ade20k import SegmentationModule, NUM_CLASS, segm_options\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/__init__.py\", line 1, in <module>\n","    from .base import *\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/base.py\", line 22, in <module>\n","    classes=pd.read_csv(classes_path),)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n","    parser = TextFileReader(filepath_or_buffer, **kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n","    self._engine = self._make_engine(f, self.engine)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n","    self.handles = get_handle(\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 873, in get_handle\n","    handle = open(\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/DatasetEditing/Inpaint-Anything/models/ade20k/object150_info.csv'\n","0.050083705357142856\n","/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","Namespace(prompt='', init_image='/content/sample_data/test_new/ANIANIAU/preliminaries/5_mask_background.png', init_image_2='/content/sample_data/test_new/ANIANIAU/preliminaries/5_rescaled.png', mask='/content/sample_data/test_new/ANIANIAU/preliminaries/5_mask_rescaled.png', skip_timesteps=0, local_clip_guided_diffusion=False, ddim=False, timestep_respacing='100', model_output_size=256, aug_num=8, clip_guidance_lambda=1000, range_lambda=50, lpips_sim_lambda=1000, l2_sim_lambda=10000, background_preservation_loss=False, invert_mask=True, enforce_background=True, seed=404, gpu_id=0, output_path='/content/sample_data/test_new/ANIANIAU', output_file='5.jpg', iterations_num=5, batch_size=1, save_video=False, export_assets=False, image_guide=False, coarse_to_fine=True, classifier_scale=0.0, y=0, class_cond=False, background_complex=15.0, final_save_root='/content/sample_data/test_new/ANIANIAU', hard=True, random_position=True, rotate_obj=False, angle=0)\n","Using device: cuda:0\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\n","/content/DatasetEditing/editing_diffusion/optimization/image_editor.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(\"/content/DatasetEditing/editing_diffusion/checkpoints/256x256_classifier.pt\", map_location=\"cpu\")\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n","/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/editing_diffusion/main.py\", line 8, in <module>\n","    image_editor.edit_image_by_prompt()\n","  File \"/content/DatasetEditing/editing_diffusion/optimization/image_editor.py\", line 188, in edit_image_by_prompt\n","    self.init_image_pil = Image.open(self.args.init_image).convert(\"RGB\")\n","  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3431, in open\n","    fp = builtins.open(filename, \"rb\")\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/sample_data/test_new/ANIANIAU/preliminaries/5_mask_background.png'\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/Inpaint-Anything/remove_anything.py\", line 5, in <module>\n","    from lama_inpaint import inpaint_img_with_lama\n","  File \"/content/DatasetEditing/Inpaint-Anything/lama_inpaint.py\", line 19, in <module>\n","    from saicinpainting.evaluation.utils import move_to_device\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/__init__.py\", line 6, in <module>\n","    from saicinpainting.evaluation.losses.base_loss import SSIMScore, LPIPSScore, FIDScore\n","  File \"/content/DatasetEditing/Inpaint-Anything/saicinpainting/evaluation/losses/base_loss.py\", line 13, in <module>\n","    from models.ade20k import SegmentationModule, NUM_CLASS, segm_options\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/__init__.py\", line 1, in <module>\n","    from .base import *\n","  File \"/content/DatasetEditing/Inpaint-Anything/models/ade20k/base.py\", line 22, in <module>\n","    classes=pd.read_csv(classes_path),)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n","    parser = TextFileReader(filepath_or_buffer, **kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n","    self._engine = self._make_engine(f, self.engine)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n","    self.handles = get_handle(\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 873, in get_handle\n","    handle = open(\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/DatasetEditing/Inpaint-Anything/models/ade20k/object150_info.csv'\n","0.07967952806122448\n","Traceback (most recent call last):\n","  File \"/content/DatasetEditing/editing_diffusion/main.py\", line 1, in <module>\n","    from optimization.image_editor import ImageEditor\n","  File \"/content/DatasetEditing/editing_diffusion/optimization/image_editor.py\", line 7, in <module>\n","    from utils.fft_pytorch import HighFrequencyLoss\n","  File \"/content/DatasetEditing/editing_diffusion/utils/fft_pytorch.py\", line 16, in <module>\n","    import torchvision.transforms as transforms\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\", line 10, in <module>\n","    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/__init__.py\", line 2, in <module>\n","    from .convnext import *\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\", line 8, in <module>\n","    from ..ops.misc import Conv2dNormActivation, Permute\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/__init__.py\", line 23, in <module>\n","    from .poolers import MultiScaleRoIAlign\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/poolers.py\", line 10, in <module>\n","    from .roi_align import roi_align\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/roi_align.py\", line 7, in <module>\n","    from torch._dynamo.utils import is_compile_supported\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\", line 2, in <module>\n","    from . import convert_frame, eval_frame, resume_execution\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 39, in <module>\n","    from torch.fx.experimental.symbolic_shapes import (\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/experimental/symbolic_shapes.py\", line 64, in <module>\n","    from torch.utils._sympy.functions import (\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_sympy/functions.py\", line 6, in <module>\n","    import sympy\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\", line 200, in <module>\n","    from .geometry import (Point, Point2D, Point3D, Line, Ray, Segment, Line2D,\n","  File \"/usr/local/lib/python3.10/dist-packages/sympy/geometry/__init__.py\", line 14, in <module>\n","^C\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-c1f168e028f6>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         )\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# set image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msam_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m# box: normalized box xywh -> unnormalized xyxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DatasetEditing/SAM/segment_anything/predictor.py\u001b[0m in \u001b[0;36mset_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0minput_image_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_image_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_torch_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DatasetEditing/SAM/segment_anything/predictor.py\u001b[0m in \u001b[0;36mset_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_image_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DatasetEditing/SAM/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DatasetEditing/SAM/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_hw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Reverse window partition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DatasetEditing/SAM/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rel_pos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_decomposed_rel_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/DatasetEditing/SAM/segment_anything/modeling/image_encoder.py\u001b[0m in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mq_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0mk_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0mRh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rel_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_pos_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mRw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rel_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_pos_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(DEVICE)"],"metadata":{"id":"CcR5pH2iljSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2U525-60SbXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"toTBn9UhJWbM"},"execution_count":null,"outputs":[]}]}